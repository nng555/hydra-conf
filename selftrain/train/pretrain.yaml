# @package _group_ 
adam_betas: (0.9, 0.98)
adam_eps: 1.0e-06
arch: ${fairseq.model._name}
attention_dropout: 0.1
augment: ~
best_checkpoint_metric: loss
clip_norm: 0.0
criterion: masked_lm
data_seed: ~
ddp_backend: no_c10d
dropout: 0.1
find_unused_parameters: true
fp16: true
fp16_init_scale: 4
fp16_scale_window: 128
init_token: ~
input0_mask: false
input1_mask: false
keep_interval_updates: 1
keep_last_epochs: 1
layer_seed: ~
leave_unmasked_prob: 0.0
log_interval: 10
lr: 0.0005
lr_scheduler: polynomial_decay
mask_prob: 0.15
max_epoch: 100
max_positions: 512
max_sentences: 16
max_source_positions: ~
max_tokens: 4400
maximize_best_checkpoint_metric: ~
num_classes: ${data.bin.num_classes}
optimizer: adam
ordinal: false
random_token_prob: 0.0
recon_model_path: ~
regression_target: false
required_batch_size_multiple: 1
reset_dataloader: ~
reset_meters: ~
reset_optimizer: ~
restore_file: ~
sample_break_mode: complete
save_interval_updates: 500
seed: ~
separator_token: ~
skip_invalid_size_inputs_valid_test: true
soft_labels: false
st_model_path: ~
task: masked_lm
threshold_loss_scale: 1
tokens_per_sample: 512
total_num_update: 3100
update_freq: 16
warmup_updates: 30
weight_decay: 0.01
