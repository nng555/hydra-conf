# @package _group_ 
adam_betas: (0.9, 0.98)
adam_eps: 1.0e-06
arch: ${model._name}
augment: ~
best_checkpoint_metric: accuracy
clip_norm: 0.0
consistency: ~
criterion: sentence_prediction
data_seed: ~
ddp_backend: no_c10d
dropout: ${model.dropout}
epoch_mask_rate: ~
find_unused_parameters: true
fp16: true
fp16_init_scale: 4
fp16_scale_window: 128
init_token: 0
input0_mask: false
input1_mask: false
keep_interval_updates: 1
keep_last_epochs: 10
layer_seed: ~
leave_unmasked_prob: 0.0
log_interval: 10
lr: ${model.lr}
lr_scheduler: ${model.lr_scheduler}
mask_prob: ~
mask_whole_words: False
max_epoch: 10
max_mask_rate: ~
max_positions: ${model.max_positions}
max_sentences: 8
max_source_positions: 512
max_tokens: 4400
maximize_best_checkpoint_metric: true
num_classes: ${data.num_classes}
optimizer: adam
ordinal: false
pooler_dropout: ${model.pooler_dropout}
random_token_prob: 0.0
recon_model_path: ~
regression_target: ${data.regression_target}
required_batch_size_multiple: 1
reset_checkpoint_heads: ~
reset_dataloader: ~
reset_meters: ~
reset_optimizer: ~
restore_file: ~
save_interval_updates: 1500
seed: ~
separator_token: 2
sigmoid_target: False
skip_invalid_size_inputs_valid_test: true
soft_labels: false
st_model_path: ~
st_unlabelled_only: ~
task: sentence_prediction
threshold_loss_scale: 1
total_num_update: ${model.total_num_update}
unlabelled_data: ~
unlabelled_augment: ~
update_freq: ${model.update_freq}
warmup_updates: eval:int((${data.num_sents}/(int("${slurm.gres}"[-1]) * ${train.max_sentences} * ${train.update_freq})) * 0.6)

